{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5Olrc0NA5hf"
      },
      "source": [
        "# California House Price Prediction Challenge\n",
        "\n",
        "**Regression challenge**\n",
        "\n",
        "Your challenge is to develop a machine learning model for predicting house prices in California using features such as the number of rooms and the age of the house.\n",
        "\n",
        "This is a great opportunity to experiment with and learn about a number of core concepts in machine learning, using [pandas](https://pandas.pydata.org/), [seaborn](https://seaborn.pydata.org/) and [scikit-learn](https://scikit-learn.org/stable/index.html).\n",
        "\n",
        "This Jupyter notebook will guide you through the various general stages involved in machine learning projects &ndash; including data visualisation, data preprocessing, model selection, model training and model evaluation &ndash; in the context of this challenge, and afterwards, you will be able to submit your test set predictions for evaluation on the [DOXA AI](https://doxaai.com/competition/palmer-penguins) platform.\n",
        "\n",
        "**Before you get started, make sure to [sign up for an account](https://doxaai.com/sign-up) if you do not already have one and [enrol to take part](https://doxaai.com/competition/california-housing) in the challenge.**\n",
        "\n",
        "**If you have any questions, feel free to ask them in the [DOXA Community Discord server](https://discord.gg/MUvbQ3UYcf).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSsq2alDA5hh"
      },
      "source": [
        "## The machine learning workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE8Abe8XA5hh"
      },
      "source": [
        "## Installing and importing useful packages\n",
        "\n",
        "To get started, we will install and import a few packages we will need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_HaoNKXA5hh"
      },
      "outputs": [],
      "source": [
        "%pip install numpy pandas matplotlib seaborn scikit-learn\n",
        "%pip install -U doxa-cli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBiwWl33A5hi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTJy3Q1GA5hi"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "The data for this challenge was originally drawn from the [1990 US census](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html).\n",
        "\n",
        "We can get started by downloading the data if we do not have it already.\n",
        "\n",
        "- `data/train.csv` is a CSV (comma-separated values) file containing the full training dataset (including both the features and the target `median_house_value` variable)\n",
        "- `data/test.csv` is a CSV file containing the test set for which your final model will be used to make predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzeW4ikbA5hi"
      },
      "outputs": [],
      "source": [
        "# Download the dataset if we do not already have it\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.makedirs(\"data\")\n",
        "\n",
        "    !curl https://raw.githubusercontent.com/DoxaAI/educational-challenges/main/california-housing/data/train.csv --output data/train.csv\n",
        "    !curl https://raw.githubusercontent.com/DoxaAI/educational-challenges/main/california-housing/data/test.csv --output data/test.csv\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv(\"data/train.csv\")\n",
        "test_df = pd.read_csv(\"data/test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaqUtuQ1A5hi"
      },
      "source": [
        "## Exploring the data\n",
        "\n",
        "Before we dive into training machine learning models, it is incredibly important to first take the time to properly explore the data available and understand its characteristics. As part of this, we will be looking to identify the following: (i) whether the dataset contains any missing, corrupted values or otherwise invalid values; (ii) whether there may be any large outliers or anomalies in the dataset (e.g. typos); (iii) how the different feature variables are distributed; and (iv) what relationships there are between the different data variables.\n",
        "\n",
        "This will guide us as to what we should do next and may even give us clues as to which data preprocessing techniques and model types we may wish to use. After all, if the quality of a dataset is low, you may have to spend some time (or a lot of time ðŸ˜…) [cleaning](https://en.wikipedia.org/wiki/Data_cleansing) it before it can be used.\n",
        "\n",
        "In this notebook, we will use some simple statistical methods and common visualisation techniques to explore the data we have.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7mttWVqA5hi"
      },
      "source": [
        "### The training dataset\n",
        "\n",
        "Let's get started by taking a look at the training dataset, which contains the following data variables:\n",
        "\n",
        "- `median_income`: the median income in block group in thousands of dollars\n",
        "- `house_age`: the median house age in block group\n",
        "- `mean_rooms`: the mean number of rooms per household\n",
        "- `mean_bedrooms`: the mean number of bedrooms per household\n",
        "- `population`: the block group population\n",
        "- `mean_household_size`: the median household size of the block\n",
        "- `latitude`: the latitude of the block group\n",
        "- `longitude`: the longitude of the block group\n",
        "- `median_house_value`: the median house value in thousands of dollars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eop3LRTQA5hi"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the first ten rows\n",
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmUvq-f_A5hj"
      },
      "source": [
        "So far, so good &ndash; nothing looks particularly outlandish, so we can continue!\n",
        "\n",
        "As part of our analysis, it can be useful to find out the following things: what columns we have; what their datatypes are; how many entries there are; and whether there are any missing values in our training dataset we might have to handle (e.g. by dropping the rows they are in or imputing the missing values). One way to do this is to use the `info()` method on our training dataframe `train_df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3EAHp60A5hj"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU3DSYDVA5hj"
      },
      "source": [
        "Fortunately, it looks like the training set is complete! From this, we can see that it is formed of **10,320 entries**, where all of our features are numeric.\n",
        "\n",
        "We can also look at some statistical properties of the numerical data using the `describe()` method on our training dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMKfNA1GA5hj"
      },
      "outputs": [],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MapP-8iPA5hj"
      },
      "source": [
        "Looking at the `75%` and `max` rows, we can already see from this table that there are likely a few extreme examples in the dataset we should be aware of, e.g. some houses with over 50 rooms.\n",
        "\n",
        "Out of curiosity, let's take a look at those instances:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub-KWPYPA5hj"
      },
      "outputs": [],
      "source": [
        "train_df[train_df[\"mean_rooms\"] > 50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Fq9z_pA5hj"
      },
      "source": [
        "A more visual way to get an intuitive feel for how the data is distributed is to plot a histogram for each variable:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvIDD8saA5hj"
      },
      "outputs": [],
      "source": [
        "train_df.hist(bins=50, figsize=(10, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGfSSYDjA5hj"
      },
      "source": [
        "Showing the data in this way makes it clear that median house price values have actually been capped at $500,000, which may have consequences depending on the application.\n",
        "\n",
        "Using [seaborn](https://seaborn.pydata.org/), we can also see this with a violin plot (which is like a fancier box plot!):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EysYK8mOA5hj"
      },
      "outputs": [],
      "source": [
        "sns.violinplot(data=train_df, x=\"median_house_value\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmAIivO7A5hj"
      },
      "source": [
        "Using [pairplot()](https://seaborn.pydata.org/generated/seaborn.pairplot.html), we can show the distribution of each numerical variable, as well as the relationships between them, all in one diagram: on the diagonal, we will plot a histogram for each numerical variable as before, but for the off-diagonal graphs, we can generate a scatter plot for each pair of numerical variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLo7FXU0A5hj"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(\n",
        "    train_df,\n",
        "    vars=[\n",
        "        \"median_income\",\n",
        "        \"house_age\",\n",
        "        \"mean_rooms\",\n",
        "        \"mean_bedrooms\",\n",
        "        \"population\",\n",
        "        \"mean_household_size\",\n",
        "        \"median_house_value\",\n",
        "    ],\n",
        "    plot_kws={\n",
        "        \"alpha\": 0.75\n",
        "    },  # make the points slightly transparent to be easier to see\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-uSuMGcA5hj"
      },
      "source": [
        "Seaborn also lets us generate other graphs, such as a scatter plot showing the location of all the houses in the dataset, where each point is coloured according to the median house price in the block group.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRB1FjCnA5hj"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(\n",
        "    train_df,\n",
        "    x=\"longitude\",\n",
        "    y=\"latitude\",\n",
        "    hue=\"median_house_value\",\n",
        "    alpha=0.75,\n",
        "    palette=\"viridis\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msye6b8VA5hj"
      },
      "source": [
        "Great! We are starting to build up a picture of what our data looks like and develop an idea of what we might want to do next.\n",
        "\n",
        "**[EXERCISE]** What other statistics and visualisations might you want to look at for the training set?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFFk_qArA5hj"
      },
      "source": [
        "## The test dataset\n",
        "\n",
        "Before we move onto preprocessing the data, take a quick look at the test dataset for which we will be making predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJPIW3VyA5hj"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnK53S_aA5hj"
      },
      "outputs": [],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3orK60DoA5hj"
      },
      "outputs": [],
      "source": [
        "test_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcOKakE0A5hj"
      },
      "source": [
        "**[EXERCISE]** What other statistics and visualisations might you want to look at for the test set?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt0dhYYKA5hk"
      },
      "source": [
        "## Preprocessing the data\n",
        "\n",
        "Now it is your turn &ndash; can you preprocess the data to get it ready for training?\n",
        "\n",
        "When preprocessing and transforming datasets, we generally seek to do the following things where applicable:\n",
        "\n",
        "- **Handling missing values**\n",
        "\n",
        "  Most machine learning models cannot be trained on missing values (often encoded as `NULL` or `NaN`), so we need a strategy to deal with them. The simplest such strategy (which we will do) is to ignore the rows that contain missing values; however, sometimes, you can lose a lot of useful data this way. Another simple strategy is that of **univariate imputation** where you replace the missing values with some descriptive statistic computed from the remaining values in that column (e.g. the mean, median or mode) or even a sensible constant value. Some more advanced ways to impute missing values include [multivariate feature imputation](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) and [nearest neighbours imputation](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer).\n",
        "\n",
        "  There is a great guide in the [scikit-learn documentation](https://scikit-learn.org/1.5/modules/impute.html) if you want to learn more!\n",
        "\n",
        "- **Scaling numerical data**\n",
        "\n",
        "  Many machine learning algorithms (especially those that use gradient descent in optimisation) do not perform as well and may even struggle to converge on a good solution if the input features all have drastically different scales. Two common ways to deal with this are (i) **normalisation** or **min-max scaling** (where data is shifted and rescaled into the range `[0, 1]` typically) and (ii) **standardisation** where the data is shifted to have a mean of zero and rescaled to have unit variance. Some approaches may be more suitable than others, depending on the models you are using and what you want to achieve, but it can often be a good idea to experiment to see what gives you better results!\n",
        "\n",
        "- **Encoding categorical data**\n",
        "\n",
        "  We usually need to encode categorical data before we can train a model on it such as by using a [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), which effectively creates a binary column for each discrete category.\n",
        "\n",
        "- **Feature engineering**\n",
        "\n",
        "  Sometimes, it is possible to transform the raw input features you have into a set of features that are more useful in training. This is beyond the scope of this tutorial, but you may wish to read into it!\n",
        "\n",
        "If you are interested in learning more about data preprocessing, check out the [scikit-learn documentation](https://scikit-learn.org/stable/modules/preprocessing.html) on the subject. Just note that you will want to preprocess the test set in the same way, so if you rescale your training data, you will want to rescale your test data in exactly the same way (with the parameters computed from the _training_ data)!\n",
        "\n",
        "**Beware of data leakage**: while you must preprocess your training, validation and test data the same way, make sure you do not leak information about the your validation and test data when doing this (e.g. by standardising your data using a mean and standard deviation calculated from all of the data, rather than just the training data), as this may artificially boost the performance of your model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49YfXxTuA5hk"
      },
      "source": [
        "### Preprocessing the training data\n",
        "\n",
        "Given that our data is entirely numerical and there are no missing values, for the time being, we will just select the features we want to use and rescale them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4PSKUzhA5hk"
      },
      "outputs": [],
      "source": [
        "# We will only use a subset of the features to begin with, but feel free to experiment!\n",
        "FEATURES = [\n",
        "    # \"median_income\",\n",
        "    \"house_age\",\n",
        "    \"mean_rooms\",\n",
        "    \"mean_bedrooms\",\n",
        "    \"population\",\n",
        "    \"mean_household_size\",\n",
        "    # \"latitude\",\n",
        "    # \"longitude\",\n",
        "]\n",
        "\n",
        "X_train = train_df[FEATURES]\n",
        "y_train = train_df[\"median_house_value\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wVj_WB2A5hk"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    # Feel free to experiment with different preprocessing techniques here! ðŸ‘€\n",
        "    (\n",
        "        # [EXERCISE] Is this the best scaler to use in this case given what we have seen?\n",
        "        MinMaxScaler(),\n",
        "        FEATURES,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9VYJcX2A5hk"
      },
      "source": [
        "## Training and evaluating models\n",
        "\n",
        "### A little theory\n",
        "\n",
        "Our objective is to develop a model with the lowest possible **generalisation error** (or **out-of-sample error**), which is a measure of how well our model performs on unseen data. This is important if we ever want to use the models we train for inference in the real world. Of course, we cannot compute this directly &ndash; the data is unseen! &ndash; so we use the **empirical error** from evaluating the model on a test set, which is not used in training at all, as a proxy. This is what we show on the [competition scoreboard](https://doxaai.com/competition/california-housing/scoreboard).\n",
        "\n",
        "In the ideal scenario, our model will generalise to perform similarly on both the training and test sets; however, this is not always the case. If our model starts to fit to the noise (or residual variation) in our training dataset, rather than the underlying function we are trying to learn (the signal), we end up **overfitting**; and likewise, if the representation of our model is not rich enough to encode that underlying relationship in the data, our model ends up **underfitting**. Both issues cause our model to perform worse when evaluated out-of-sample.\n",
        "\n",
        "There are a lot of different models available to us (with different suitabilities for encoding different relationships), each with a range of **hyperparameters** we can tune to affect the learning process; however, if we use the performance of our models on the test set as the basis for updating hyperparameters, we stand the risk of leaking information about and overfitting to the test set.\n",
        "\n",
        "One approach is to further subdivide our training dataset into a training set and a completely separate **validation set**, but training data is precious and in short supply a lot of the time, and we would not want to overfit to that validation set too. An alternative approach here is to perform (**stratified**) **k-fold cross-validation**, where we (randomly) partition the data into `k` different \"folds\", train `k` models using each fold for validation and the remaining folds for training, and average the results. You can read more about this in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
        "\n",
        "Putting this all together, we finally just need a strategy for optimising our hyperparameters. One (albeit potentially slow) way is just to perform a **cross-validated grid search** over a grid of hyperparameter values we want to investigate in order to try out a range of different combinations and see what performs well. When we think we have found the model with the best set of hyperparameters, we can retrain it on all of the training data and then evaluate it on the test set! ðŸ˜Ž\n",
        "\n",
        "### Putting everything into practice\n",
        "\n",
        "We can now try applying what we know by performing a small grid search of our own on a [support vector machine with a linear kernel](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html) (or `LinearSVR`) to find the value of the hyperparameter `C` (which controls the strength of regularisation that is applied) that gives the best performing model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "945A2v5_A5hk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "\n",
        "parameter_grid = {\n",
        "    # Try experimenting with other parameters!\n",
        "    \"model__C\": [0.5, 1, 2],\n",
        "}\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    [\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"model\", LinearSVR()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "regressor = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=parameter_grid,\n",
        "    cv=5,  # Use 5 folds\n",
        "    refit=True,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        ")\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", regressor.best_params_)\n",
        "print(\"Best mean absolute error:\", -regressor.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWZeD6i-A5hk"
      },
      "source": [
        "Neat! We just used [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to perform a cross-validated grid search and automatically retrain the model on all of the training data, which we can now use to make predictions for the test set! ðŸ¥³\n",
        "\n",
        "**[EXERCISE] Since we do all our preprocessing as part of the pipeline we have made, it will be repeated every time a model is trained for a particular fold. Why might this be what we want?**\n",
        "\n",
        "**[EXERCISE] What other models could you use for this regression task?**\n",
        "\n",
        "**Hint**: take a look at the [scikit-learn documentation](https://scikit-learn.org/1.5/supervised_learning.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHEI9uKmA5hk"
      },
      "source": [
        "## Submitting our predictions to the DOXA AI platform\n",
        "\n",
        "We are now ready to generate house price predictions for the test set and upload our work to the DOXA AI platform for evaluation!\n",
        "\n",
        "**Make sure to [enrol to take part](https://doxaai.com/competition/california-housing) in the challenge if you have not already done so.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABQ3g-j7A5hk"
      },
      "outputs": [],
      "source": [
        "predictions = regressor.predict(test_df[FEATURES])\n",
        "\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RPPsTbuA5hn"
      },
      "outputs": [],
      "source": [
        "# Prepare our submission\n",
        "\n",
        "os.makedirs(\"submission\", exist_ok=True)\n",
        "\n",
        "with open(\"submission/doxa.yaml\", \"w\") as f:\n",
        "    f.write(\n",
        "        \"competition: california-housing\\nenvironment: cpu\\nlanguage: python\\nentrypoint: run.py\\n\"\n",
        "    )\n",
        "\n",
        "with open(\"submission/run.py\", \"w\") as f:\n",
        "    contents = \"\\\\n\".join([str(prediction) for prediction in predictions])\n",
        "    f.write(\n",
        "        f\"\"\"import os\n",
        "with open(os.environ[\"DOXA_STREAMS\"] + \"/out\", \"w\") as f:\n",
        "    f.write(\"{contents}\")\"\"\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyi72PT9A5hn"
      },
      "source": [
        "Next, we need to make sure we are logged in:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNFt0H5fA5hn"
      },
      "outputs": [],
      "source": [
        "!doxa login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISh8TPYdA5hn"
      },
      "source": [
        "Finally, we can submit the predictions for evaluation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKBH7iZ6A5hn"
      },
      "outputs": [],
      "source": [
        "!doxa upload submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzei6RjhA5hn"
      },
      "source": [
        "Wooo! ðŸ¥³ You have just submitted your predictions to the platform &ndash; well done! Take a moment to see how well you have done on the [scoreboard](https://doxaai.com/competition/california-housing/scoreboard).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YokusJa4A5hn"
      },
      "source": [
        "## Possible improvements\n",
        "\n",
        "Congratulations &ndash; you have made it to the end! We hope you have enjoyed learning about and applying machine learning to this challenge. Hopefully, you can now start experimenting with your own ideas to see how you can improve the performance of your model!\n",
        "\n",
        "Here are a few questions and ideas to get you started:\n",
        "\n",
        "1. **Data visualisation**:\n",
        "\n",
        "- What other visualisations can you make using **pandas**, **matplotlib** and **seaborn**?\n",
        "\n",
        "2. **Data preprocessing**:\n",
        "\n",
        "- What other features might you want to use? Is there a way to select them automatically?\n",
        "- How might you scale the data differently to boost performance?\n",
        "- Would applying the [PCA algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) help here?\n",
        "\n",
        "3. **Model selection**:\n",
        "\n",
        "- What alternatives are there to running a standard grid search? Take a look at [HalvingGridSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV), [HalvingRandomSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV) and [RandomizedSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)!\n",
        "- Have you ever tried using [BayesSearchCV](https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html) from the [scikit-optimize](https://scikit-optimize.github.io/stable/) package? It searches the space of hyperparameters using Bayesian optimisation and is a drop-in replacement for [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
        "- What other models could you use for this regression task? Take a look at the [scikit-learn documentation](https://scikit-learn.org/1.5/supervised_learning.html)!\n",
        "- Could you improve performance by using an [ensemble of different models](https://scikit-learn.org/stable/modules/ensemble.html)?\n",
        "\n",
        "## Closing remarks\n",
        "\n",
        "We hope that you have found this to be a useful and enjoyable exercise in exploring and gaining exposure to some fascinating ideas and concepts in machine learning. We look forward to seeing what you build! Do continue the conversation on the [DOXA Community Discord server](https://discord.gg/MUvbQ3UYcf). ðŸ˜Ž\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}