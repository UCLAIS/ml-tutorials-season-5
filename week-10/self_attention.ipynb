{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Attention is all you need**\n",
    "\n",
    "## https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The mathematical trick in self-attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.ones(3,3)\n",
    "\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Using torch.tril()\n",
    "\n",
    "We use torch.tril() to get the lower triangular part of a matrix. \n",
    "\n",
    "torch.tril() creates a lower triangular matrix by keeping all elements on and below the main diagonal while setting everything above it to zero. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "                  \n",
    "print(torch.tril(x))\n",
    "# Output:\n",
    "# [[1, 0, 0],\n",
    "#  [4, 5, 0],\n",
    "#  [7, 8, 9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "\n",
    "# Go through the matrix multiplication step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the weighted average\n",
    "This makes the future tokens attentive to the past tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# keep eveything the same but divide a to make it apply a weighted average\n",
    "\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the same trick with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take an example \n",
    "import torch.nn.functional as F # Importing the module for softmax\n",
    "\n",
    "torch.manual_seed(42)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "\n",
    "# Batch represents the number of sequences in the batch\n",
    "# Time represents the number of tokens in the sequence\n",
    "# Channels represents the number of features in the input\n",
    "\n",
    "# This reuslts in a tensor of shape (B, T, C)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "\n",
    "# Apply the same trick however this time we use the softmax function\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "#wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "wei\n",
    "\n",
    "# xbow = wei @ x # bow --> bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "---\n",
      "torch.Size([4, 8, 2])\n",
      "tensor([[ 1.4873, -0.3091, -0.6176, -0.8644, -0.3617, -0.5354, -0.5388, -0.3762],\n",
      "        [-0.1596,  0.1400,  0.4528,  0.7597,  0.8671,  0.9450,  0.8160,  0.8215],\n",
      "        [-0.8712,  0.4231,  0.1405, -0.0882,  0.1285,  0.0069,  0.3092,  0.2095],\n",
      "        [-0.6581, -0.0662,  0.3530,  0.0808,  0.0718,  0.1724,  0.4113,  0.5329]])\n"
     ]
    }
   ],
   "source": [
    "# Lets take an example \n",
    "import torch.nn.functional as F # Importing the module for softmax\n",
    "\n",
    "torch.manual_seed(42)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "\n",
    "# Batch represents the number of sequences in the batch (e.g. the number of letters (or text sequences) in the batch)\n",
    "# Time represents the number of tokens in the sequence (e.g. the number of letters in the sequence)\n",
    "# Channels represents the number of features in the input (e.g. the vector that represents the token/word)\n",
    "\n",
    "# This reuslts in a tensor of shape (B, T, C)\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "\n",
    "# Apply the same trick however this time we use the softmax function\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T)) # Set initially to 0, during training this will be the attention matrix that we will learn based on the data\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # Set the upper triangular part to -inf, so the tokens from the future are can not communicate\n",
    "wei = F.softmax(wei, dim=-1) # Apply the softmax function to normalize the weights\n",
    "\n",
    "# bow --> bag of words\n",
    "xbow = wei @ x # Then we apply the attention matrix to the input to get the output\n",
    "\n",
    "print(wei)\n",
    "print('---')\n",
    "print(xbow.size())\n",
    "print(xbow[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does each dimension represent?**\n",
    "\n",
    "- B: Batch size\n",
    "- T: Time (number of tokens in the sequence)\n",
    "- C: Channels (number of features in the input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize we create an attention matrix which averages the input based on the past tokens on the current token.\n",
    "\n",
    "#### Applying the query and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# In the code cell above our weights are uniform i.e. all the previous (and current) tokens are given equal weight in producing the output.\n",
    "# we will learn how to create an attention matrix that is not uniform.\n",
    "\n",
    "\n",
    "# New toy model with a larget channel dimension\n",
    "torch.manual_seed(42)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# We implement a single head of self attention.\n",
    "head_size = 16 # Project the input to a smaller dimension\n",
    "\n",
    "# Do not include bias\n",
    "# Only the relationship between the query and key is important for the attention mechanism\n",
    "# Not their relative difference\n",
    "query = nn.Linear(C, head_size, bias=False) \n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size) -> (B, T, 16)\n",
    "q = query(x) # (B, T, head_size) -> (B, T, 16)\n",
    "\n",
    "## 3.2.1 Scaled Dot-Product Attention from the paper\n",
    "\n",
    "# The wei matrix gives us the relationship between the each of the tokens in the sequence\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3332, -0.6597,  0.3630, -0.1001,  0.0136, -0.5833,  1.1351, -1.2784],\n",
       "        [-1.1723,  0.7869, -1.5219,  0.8649, -1.6202,  1.2025, -1.9940, -0.4554],\n",
       "        [-1.0216, -1.2725,  0.7821, -0.0335, -1.9888, -0.3281,  1.5545, -1.4118],\n",
       "        [-0.0545,  1.6851, -1.7215,  1.0221, -0.3327,  0.9147, -1.8037,  0.6392],\n",
       "        [-1.0950,  0.1159, -0.3494, -0.1350, -1.2506,  0.9809, -0.5062, -0.5780],\n",
       "        [ 0.2735,  0.5450,  0.2884, -0.3078, -0.8928, -0.4859, -2.6109,  1.9291],\n",
       "        [ 0.1340,  0.2356, -0.1021,  0.1440, -2.2674,  1.7589, -1.0739,  1.6689],\n",
       "        [-0.8490, -0.1962, -1.4271, -0.3019,  3.0561,  0.1650,  1.6430,  0.1103]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are we missing something?\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, we need to ensure that the past tokens do not see the future tokens\n",
    "# We have already done this above\n",
    "\n",
    "# CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[ 1.9269,  1.4873],\n",
      "        [ 1.4138, -0.3091],\n",
      "        [ 1.1687, -0.6176],\n",
      "        [ 0.8657, -0.8644],\n",
      "        [ 0.5422, -0.3617],\n",
      "        [ 0.3864, -0.5354],\n",
      "        [ 0.2272, -0.5388],\n",
      "        [ 0.1027, -0.3762]])\n"
     ]
    }
   ],
   "source": [
    "# Lets Check the output\n",
    "print(xbow.shape)\n",
    "print(xbow[0])\n",
    "\n",
    "# We can see that the wei matrix is now a lower triangular matrix\n",
    "# This means that the past tokens are only able to see the past tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the value\n",
    "\n",
    "So you probably have heard about the query, key and value. But currently we have only used the query and key.\n",
    "\n",
    "So what is the value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "tensor([-0.3704, -0.0500,  0.0160,  0.0762,  0.0193,  0.0577,  0.0694,  0.0524],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We create a new linear layer to project the input to the value\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "v = value(x) # (B, T, C) -> (B, T, 16)\n",
    "\n",
    "# We apply the attention matrix to the value instead of the raw x\n",
    "vbow_out = wei @ v # (B, T, T) @ (B, T, 16) -> (B, T, 16)\n",
    "\n",
    "# Note you have to implement the triangulation mask above for wei\n",
    "print(vbow_out.shape)\n",
    "print(vbow_out[0,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "1. Batches do not communicate with each other.\n",
    "2. For some applications like sentiment analysis, we do not need to mask the future tokens since we want all the tokens to talk to each other fully. So in this case we use the encoder block, where we remove the mask. In 'decoder' blocks we keep the mask, because we want to model language.\n",
    "3. Self attention vs cross attention:\n",
    "    - Self attention: the query, key and value are all coming from the same source.\n",
    "    - Cross attention: the query, key and value are all coming from different sources. For example in transformers queries can be produced from x and keys and values can be produced from the encoder, a whole seperate source.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer to the forward method in the **gpt.py** file for the full implementation\n",
    "\n",
    "There are many additional optimizations which we have not shown you in our tutorial, but is implemented in the **gpt.py** file. In his video Dr.Karpathy discusses:\n",
    "\n",
    "1. Multi-head attention\n",
    "2. Feed-forward layer structure in transformers\n",
    "3. Residual connections\n",
    "4. Layer and batch normalization\n",
    "5. Scaling up the model (which is why you may not be able to run the code on your own machine)\n",
    "6. General concepts not used by him but used in industry like RLHF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_vocal_coach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
