{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Classification - Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a brief introduction to **Support Vector Machines** (SVMs)\n",
    "\n",
    "We will show two examples. One after using PCA to reduce the dimensionality of the data, and one with just using two features from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data # Shape: (150, 4)\n",
    "y = data.target # Shape: (150,). Note there are three classes: 0, 1, 2\n",
    "labels = data.feature_names\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### Implement a PCA that reduces the dataset to 2 dimensions.\n",
    "pca = None\n",
    "pca_2d = None\n",
    "###\n",
    "\n",
    "# Two features\n",
    "X_two_features = X[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMs using Scikit-Learn\n",
    "\n",
    "Our code in this section shows how Support Vector Machines (SVMs) can be used with Scikit-Learn. SVM helps us find the optimal hyperplane to separate different classes. We will demonstrate how to create, train, and evaluate an SVM model, showcasing its versatility and ability to handle complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM\n",
    "\n",
    "### Implement and train your svm using pca_2d, use random state 42.\n",
    "svm = None\n",
    "###\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    svm,\n",
    "    pca_2d,\n",
    "    response_method=\"predict\",\n",
    "    ax=ax,\n",
    "    alpha=0.8,\n",
    "    cmap=plt.cm.coolwarm\n",
    ")\n",
    "ax.scatter(pca_2d[:, 0], pca_2d[:, 1], c=y, s=75, cmap=plt.cm.coolwarm, edgecolor=\"k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM\n",
    "\n",
    "### Implement and train your svm using X_two_features, use random state 42.\n",
    "svm2f = None\n",
    "###\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    svm2f,\n",
    "    X_two_features,\n",
    "    response_method=\"predict\",\n",
    "    ax=ax,\n",
    "    alpha=0.8,\n",
    "    cmap=plt.cm.coolwarm\n",
    ")\n",
    "ax.scatter(X_two_features[:, 0], X_two_features[:, 1], c=y, s=75, cmap=plt.cm.coolwarm, edgecolor=\"k\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
