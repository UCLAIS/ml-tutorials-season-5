{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Classification - K-Means Clustering (Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will focus on **k-means clustering**, a popular unsupervised learning algorithm. We will use the k-means algorithm to cluster data points into $k$ clusters! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of X and y\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first demonstrate the use of two components of the Scikit-Learn library for machine learning: clustering using K-Means and dimensionality reduction using Principal Component Analysis. The first step is to import the necessary modules, KMeans from sklearn.cluster and PCA from sklearn.decomposition. Write some that code that initializes a K-Means clustering model with three clusters. Then, fit the K-Means model to the data represented by the variable X.\n",
    "\n",
    "A PCA is used to reduce the data's dimensionality to 2D to create a scatter plot to visualize the data points, each coloured according to its cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering Using Scikit-Learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### Initialize KMeans model and train it.\n",
    "km = None\n",
    "###\n",
    "\n",
    "# Visualize the results\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "pca_2d = pca.transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=y)\n",
    "centroids = pca.transform(km.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c=\"red\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Elbow Method\n",
    "We can use the elbow method to find the number of clusters we need. An elbow of a curve is a point where the curve visibly changes direction. In the case of the elbow method, we are looking for the point where the curve starts to flatten out. This point is the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Try different values of k\n",
    "k_range = range(1, 10)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=0)\n",
    "    km.fit(X)\n",
    "    scores.append(km.inertia_)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means By Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-means algorithm can be broken down into multiple steps. Our aim is to minimise the sum of squared distances between each data point and its corresponding cluster centroid. This equation can be written as: \n",
    "\n",
    "$$\\Theta = \\sum_{i}^{n} \\sum_{j}^{k} \\rho^{i[j]} ||x^{i} - \\mu^{[j]}||^2_2$$\n",
    "\n",
    "where $n$ is the number of data points, $k$ is the number of clusters, $x^{i}$ is the $i^{th}$ data point, $\\mu^{[j]}$ is the $j^{th}$ cluster centroid, and $\\rho^{i[j]}$ is the indicator function which is equal to 1 if $x^{i}$ belongs to cluster $j$ and 0 otherwise.\n",
    "\n",
    "The algorithm follows the following format:\n",
    "\n",
    "1. Initialise cluster centroids $\\mu^{j}$ for $j = 1, ..., k$.\n",
    "2. **E-Step**: Minimise $\\Theta$ with respect to $\\rho^{i[j]}$ for $i = 1, ..., n$ and $j = 1, ..., k$, keeping $\\mu^{[j]}$ fixed.\n",
    "3. **M-Step**: Minimise $\\Theta$ with respect to $\\mu^{[j]}$ for $j = 1, ..., k$, keeping $\\rho^{i[j]}$ fixed.\n",
    "4. Repeat steps 2 and 3 until convergence.\n",
    "\n",
    "\n",
    "### E-Step\n",
    "Each term is independent of each other so we can minimise $\\Theta$ with respect to $\\rho^{i[j]}$ for $i = 1, ..., n$ and $j = 1, ..., k$ separately.\n",
    "We can use the following formula to calculate $\\rho^{i[j]}$:\n",
    "\n",
    "$$\\rho^{i[j]} = \\begin{cases} \n",
    "      1 & \\text{if } j = \\text{argmin}_{l} ||x^{i} - \\mu^{[l]}||^2_2 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "   \\end{cases}$$\n",
    "\n",
    "Ties can be broken arbitrarily.\n",
    "\n",
    "### M-Step\n",
    "We can take the derivative of $\\Theta$ with respect to $\\mu^{[j]}$ and set it to 0 to find the optimal $\\mu^{[j]}$. Doing this, and rearranging, we get:\n",
    "\n",
    "$$\\mu^{[j]} = \\frac{\\sum_{i}^{n} \\rho^{i[j]} x^{i}}{\\sum_{i}^{n} \\rho^{i[j]}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the code below, please fill in the line that computes the distance between each data point and the centroid. Below the E-step, implement the M-step which updates the centroid position to be the mean location of the cluster data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k, max_iter=1000, random_state=42):\n",
    "    \n",
    "    # Step 1: Initialize the centroids\n",
    "    np.random.seed(random_state)\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = X[np.random.choice(n_samples, k, replace=False)]\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "        # There are several numpy functions that can be used to optimize the following code\n",
    "        # however we will use the long version for the sake of clarity\n",
    "\n",
    "\n",
    "        # E-Step (Assign points to the nearest cluster centroid)\n",
    "        rhos = np.zeros((n_samples, k))\n",
    "        for i in range(n_samples):\n",
    "            distances = np.zeros(k)\n",
    "            for j in range(k):\n",
    "                ### Compute the distance here.\n",
    "                distances[j] = 0\n",
    "                ###\n",
    "            rhos[i, np.argmin(distances)] = 1\n",
    "        \n",
    "        \n",
    "        ### Implement the M-step (Update centroid location) here.\n",
    "        new_centroids = np.zeros((k, n_features))\n",
    "        ###\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the outcome of your code on a plot! How well did you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "centroids = kmeans(X, k=3)\n",
    "\n",
    "# Visualize the results\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "pca_2d = pca.transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=y)\n",
    "centroids = pca.transform(centroids)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c=\"red\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
