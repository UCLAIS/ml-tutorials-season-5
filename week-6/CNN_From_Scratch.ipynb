{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APhzWS-BTqRU"
   },
   "source": [
    "# Convolutional Neural Networks from Scratch\n",
    "\n",
    "In this notebook we are going to take a look at how to implement a simple CNN model from scratch in Python, using mostly just `numpy`.\n",
    "\n",
    "\n",
    "In practice, we can use high-level libraries such as Keras or PyTorch to abstract away the underlying details of CNN when writing code. However, we find that the exercise of writing one from scratch is very helpful in gaining a deeper understanding of CNNs and how these frameworks work under the hood.\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4NzaVImUkSm"
   },
   "source": [
    "## Convolutions\n",
    "\n",
    "We won't have a CNN without some convolutional layers! Let's start by implementing a convolutional layer class. All layers that we are going to implement in this notebook will be defined by a simple interface: all of them will have a `forward` and `backward` method for the forward and backward pass respectively. \n",
    "\n",
    "This will be easier to explain once we have some code to refer to. So here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lLtEGLqRUOnU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Conv:\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, filter_size, pad=0, stride=1, weight_scale=1e-3\n",
    "    ):\n",
    "        self.pad = pad\n",
    "        self.stride = stride\n",
    "        # Weights initialized from 0-centered Gaussian with standard deviation\n",
    "        # equal to `weight_scale` (by default set to 1e-3) for stability\n",
    "        self.filters = (\n",
    "            np.random.randn(out_channels, in_channels, filter_size, filter_size)\n",
    "            * weight_scale\n",
    "        )\n",
    "        # biases initialized to zero\n",
    "        self.bias = np.zeros((out_channels,))\n",
    "\n",
    "    def calc_output_dim(self, x):\n",
    "        _, H, W = x.shape\n",
    "        _, _, filter_height, filter_width = self.filters.shape\n",
    "\n",
    "        H_out = 1 + (H + 2 * self.pad - filter_height) // self.stride\n",
    "        W_out = 1 + (W + 2 * self.pad - filter_width) // self.stride\n",
    "\n",
    "        return H_out, W_out\n",
    "\n",
    "    def iterate_regions(self, x):\n",
    "        H_out, W_out = self.calc_output_dim(x)\n",
    "        F, C, filter_height, filter_width = self.filters.shape\n",
    "\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                x_region = x[\n",
    "                    :,\n",
    "                    i * self.stride : i * self.stride + filter_height,\n",
    "                    j * self.stride : j * self.stride + filter_width,\n",
    "                ]\n",
    "                yield i, j, x_region\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: An ndarray containing input data, of shape (C, H, W)\n",
    "        Returns:\n",
    "        - out: Result of convolution, of shape (F, H', W')\n",
    "        where F is the number of out channels and\n",
    "        H' = 1 + (H + 2 * pad - filter_height) / stride\n",
    "        W' = 1 + (W + 2 * pad - filter_width) / stride\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = x  # save as cache\n",
    "\n",
    "        _, H, W = x.shape\n",
    "        F, _, _, _ = self.filters.shape\n",
    "\n",
    "        # to pad last two dimensions of the four dimensional tensor\n",
    "        x_pad = np.pad(x, self.pad, mode=\"constant\")\n",
    "\n",
    "        H_out, W_out = self.calc_output_dim(x)\n",
    "        out = np.zeros((F, H_out, W_out))\n",
    "\n",
    "        for f in range(F):\n",
    "            # convolve F times to get F x filter_height x filter_width\n",
    "            kernel = self.filters[f]  # dimension is C x filter_height x filter_width\n",
    "            for i, j, x_region in self.iterate_regions(x_pad):\n",
    "                out[f, i, j] = (x_region * kernel).sum()\n",
    "\n",
    "            out[f] += self.bias[f]  # makes use of broadcasting\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - dout: An ndarray of the upstream derivatives, of shape (F, H', W')\n",
    "        Returns a tuple of:\n",
    "        - dx: Gradient w.r.t input x, of shape (C, H, W)\n",
    "        - dw: Gradient w.r.t. filters, of shape (F, C, filter_height, filter_width)\n",
    "        - db: Gradient w.r.t the biases, of shape (F, )\n",
    "        \"\"\"\n",
    "\n",
    "        _, H, W = self.x.shape  # retrieve from cache\n",
    "        F, C, HH, WW = self.filters.shape\n",
    "\n",
    "        x_pad = np.pad(self.x, self.pad, mode=\"constant\")\n",
    "\n",
    "        dx = np.zeros_like(x_pad)\n",
    "        dw = np.zeros_like(self.filters)\n",
    "        db = np.zeros_like(self.bias)\n",
    "\n",
    "        for f in range(F):\n",
    "            for i, j, x_region in self.iterate_regions(x_pad):\n",
    "                dw[f] += dout[f, i, j] * x_region\n",
    "\n",
    "                # -------- Task 1 : Compute dx below -----------------#\n",
    "\n",
    "                # ---------------- End of task 1 ---------------------#\n",
    "\n",
    "            # -------- Task 2 : Compute db below -----------------#\n",
    "\n",
    "            # ---------------- End of task 2 ---------------------#\n",
    "\n",
    "        # trim off padding so that dimension of dx is the same as x\n",
    "        dx = dx[:, self.pad : self.pad + H, self.pad : self.pad + W]\n",
    "\n",
    "        return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph56zru0EdBC"
   },
   "source": [
    "### Convolutional layer: forward pass\n",
    "\n",
    "In the `forward` call, we pass in the input image `x`, which will be a numpy ndarray of dimensions `C x H x W` where `C` is the number of channels (3 for RGB) and `H` and `W` are the height and weight of the image respectively.  \n",
    "\n",
    "The first thing we do is to cache the input so that we have access to it during the backpropagation:\n",
    "\n",
    "```python\n",
    " self.x = x  # save as cache\n",
    "```\n",
    "\n",
    "Following that, we pad the image with the specified padding, using 0 as the value for the padded cells. To do this, we make use of numpy's `np.pad` method:\n",
    "\n",
    "```python\n",
    "x_pad = np.pad(x, self.pad, mode='constant')\n",
    "```\n",
    "\n",
    "Following this, we define `out`, which is the tensor / ndarray we will return from `forward` as the result of the convolution. This will be a three-dimensional tensor with dimensions `F x H_out x W_out`, where `F` is the number of filters we specified. We calculate `H_out` and `W_out` using a formula which we extract out to a separate helper function, `calc_output_dim()`.  \n",
    "\n",
    "```python\n",
    "H_out, W_out = self.calc_output_dim(x)\n",
    "out = np.zeros((F, H_out, W_out))\n",
    "```\n",
    "\n",
    "Next comes the main logic for calculating the result of the convolutional operation:\n",
    "\n",
    "```python\n",
    "for f in range(F):\n",
    "      # convolve F times to get F x filter_height x filter_width \n",
    "      kernel = self.filters[f] # dimension is C x filter_height x filter_width    \n",
    "      for i, j, x_region in self.iterate_regions(x_pad):\n",
    "        out[f, i, j] = (x_region * kernel).sum()\n",
    "```\n",
    "\n",
    "For each filter, we calculate the result of multiplying the filter (`filter` is a reserved keyword in Python, so we use the variable name `kernel`) with each region in the image of the same dimension as the filter (recall the CNN theory lecture), and set the output to be returned. Here we make use of the helper function `iterate_regions()`, which is a generator that `yield`s regions of the image `x_pad` which are of the same dimensions as the kernel (keeping the stride into account, as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YQlItcaMCeP"
   },
   "source": [
    "### Convolutional layer: backward pass\n",
    "\n",
    "In the backward pass, we pass in `dout`, which are the upstream gradients which are being propagated from the next layer. In particular, `dout` is the partial derivative of our loss function (in the context of classifying digits in MNIST with CNNs, this would be the cross-entropy loss w/ softmax) with respect to the output of `forward`, which is a 3-dimensional tensor of dimensions `F x H_out x W_out`, as we previously calculated.\n",
    "\n",
    "Given `dout` and the cached input, our job is to calculate `dx`, `dw`, `db`, which are the partial derivatives of the loss with respect to the input, filters (`w` stands for weights), and the biases respectively. The dimensions of each of these will be the same as the dimensions of the variables we are taking the derivative with respect to. In `backward()`, we return these derivatives as a tuple. `dx` is used as the `dout` for the previous layer (recall how backpropagation works!), while `dw` and `db` will be used to update the filter weights and biases respectively via gradient descent. The details of how to do this are omitted from this notebook to keep it short, but the implementation should be relatively straightforward once you have these gradients.\n",
    "\n",
    "Now let's look at the backpropagation in greater detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzcD3xg1WWDq"
   },
   "source": [
    "#### Theory\n",
    "\n",
    "Let's see how we can calculate `dw` first. To recap, `dw` is the gradient of the loss w.r.t the filter weights $w$ :  $\\frac{\\partial L}{\\partial w}$.\n",
    "\n",
    "Let the output of conv (variable `out` from the forward pass) be $o$. Using the **chain rule**, we get: $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial o} * \\frac{\\partial o}{\\partial w}$. We already know $\\frac{\\partial L}{\\partial o}$ — this is the upstream gradient `dout`. Now we only need to calculate $\\frac{\\partial o}{\\partial w}$, which is the gradient of the output w.r.t to the filter weights. Let's think about how to calculate this.\n",
    "\n",
    "Imagine we have a `3 x 3` image $x$ and a `2 x 2` kernel filled with zeros. The result of the convolution operation is a `2 x 2` output also filled with zeros (rightside of the arrow). We assume no padding and a stride of 1:  \n",
    "\n",
    "<img src='https://i.imgur.com/zAVGF6p.png' width='700'/>\n",
    "\n",
    "Now let's imagine what would happen if we *nudge* the value of the top left cell of the filter, such that it becomes a 1. How would the output change? After all, this is essentially what $\\frac{\\partial o}{\\partial w}$ is telling us. Let's look at the updated diagram:\n",
    "\n",
    "<img src='https://i.imgur.com/oxfqrj0.png' width='700'/>\n",
    "\n",
    "Can you see the pattern? We can see that the output changed by exactly the image pixel values that the changed filter weight is multiplied with during the convolution operation. In other words, the derivative of a single pixel of our output $o$ with respect to a filter weight is just the corresponding pixel value in the image (that this filter weight is multiplied by). \n",
    "\n",
    "More formally, if we let $o$ to be $m$ x $n$ and the filter to be $w$ x $h$, then the $i^{th}$ row and $j^{th}$ column of the output $o$ (which we denote by $o[i,j]$) can be expressed as:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "o[i,j] = \\sum_{r=0}^{w} \\sum_{c=0}^{h} x[i + r, j + c] * filter[r,c] \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "and as we've just seen, the partial derivative of this term w.r.t a specific filter weight is given by:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial o[i,j]}{\\partial filter[r,c]} = x[i + r, j + c] \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We now have everything we need to obtain the gradient of the loss with respect to the filter weight indexed by $[r,c]$, $\\frac{\\partial L}{\\partial filter[r,c]}$, using the chain rule:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial filter[r,c]} = \\sum_{i=0}^{m} \\sum_{j=0}^{n} \\frac{\\partial L}{\\partial o[i,j]} * \\frac{\\partial o[i,j]}{\\partial filter[r,c]} = \\sum_{i=0}^{m} \\sum_{j=0}^{n} \\frac{\\partial L}{\\partial o[i,j]} * x[i + r, j + c]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We can compute the derivative of the loss w.r.t `x` similarly, where instead of the pixel value of the image it's the corresponding filter weight. We'll leave this as an exericse for you!\n",
    "\n",
    "\n",
    "Lastly, let's quickly go through how we compute `db`, the derivative of the loss w.r.t the biases. In our example, the biases are passed in as an numpy array of shape `(F, )` where `F` is the number of filters. Therefore, `db` will have the same dimension. \n",
    "\n",
    "Given a single filter, we have a scalar value `b` being added to each 'pixel' of the output. Because this is just simply addition, the derivative of a pixel of the output w.r.t to the bias term `b` ( $\\frac{\\partial o[r,c]}{\\partial b}$) is 1. This means that the derivative for the bias term for each filter is simply the sum of `dout` for that filter. \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{m} \\sum_{j=0}^{n} \\frac{\\partial L}{\\partial o[i,j]} * \\frac{\\partial o[i,j]}{\\partial b} = \\sum_{i=0}^{m} \\sum_{j=0}^{n} \\frac{\\partial L}{\\partial o[i,j]}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebVqCozJPNgz"
   },
   "source": [
    "#### Implementation \n",
    "\n",
    "Now that we've covered the theory of how backpropagation works for convolutions, let's see how we translate this over to code:\n",
    "\n",
    "After extracting the dimensions we need and padding the input, we initialize `dx`, `dw` and `db` with zeros:\n",
    "\n",
    "```python\n",
    "dx = torch.zeros_like(x_pad) \n",
    "dw = torch.zeros_like(self.filters) \n",
    "db = torch.zeros_like(self.bias)\n",
    "```\n",
    "\n",
    "For each filter, we iterate over each image region and accumulate the loss gradients for `dw` and `dx`. As we just saw, the gradient for the bias term for a particular filter is just the sum of `dout` for that filter. Note that for `dx`, we need to trim off the padding such that the dimensions of `dx` is the same as the original input we passsed into `forward()`.\n",
    "\n",
    "```python\n",
    "for f in range(F): \n",
    "  for i, j, x_region in self.iterate_regions(x_pad):\n",
    "    dw[f] += dout[f, i, j] * x_region\n",
    "\n",
    "    #-------- Task 1 : Compute dx below -----------------#\n",
    "\n",
    "    #---------------- End of task 1 ---------------------#\n",
    "\n",
    "\n",
    "  #-------- Task 2 : Compute db below -----------------#\n",
    "\n",
    "  #---------------- End of task 2 ---------------------#\n",
    "\n",
    "# trim off padding so that dimension of dx is the same as x\n",
    "dx = dx[:, self.pad: self.pad + H, self.pad : self.pad + W]\n",
    "```\n",
    "\n",
    "You may notice that we have purposefully left out the code that computes `dx` and `db`. Try implementing them before checking your answers against our solution notebook. You can run the next few cells below to quickly test out your implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x2ovXIWSpkc"
   },
   "source": [
    "#### (Aside) Quick sanity check\n",
    "\n",
    "The cells below serve as a quick sanity check to make sure that you aren't getting any errors in your implementation and that the methods are returning ndarrays of the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1636946049603,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "oRBd3_tsZmkx",
    "outputId": "b3dff35e-0c6c-4cfd-9c2e-50c0ebb05920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx shape:  (3, 28, 28)\n",
      "dw shape:  (8, 3, 3, 3)\n",
      "db shape:  (8,)\n"
     ]
    }
   ],
   "source": [
    "dout = np.random.randn(8, 26, 26)\n",
    "dx, dw, db = conv.backward(dout)\n",
    "\n",
    "print(\"dx shape: \", dx.shape)\n",
    "print(\"dw shape: \", dw.shape)\n",
    "print(\"db shape: \", db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1636946049305,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "QRH3rcg6XLK-",
    "outputId": "a7e40e36-e00d-4495-8c64-7e85d5cd0af1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 26, 26)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummy image\n",
    "img = np.random.randn(3, 28, 28)  # assume 28 x 28 RGB image\n",
    "\n",
    "conv = Conv(in_channels=3, out_channels=8, filter_size=3)\n",
    "out = conv.forward(img)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6YCcVkiWYhk"
   },
   "source": [
    "## Pooling layers : max pool\n",
    "\n",
    "Besides conv, another key component in CNNs is the pooling layers. In this notebook, we will show you how to implement a max pooling layer from scratch, again using nothing but `numpy`. Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfQlv4CAM6qO"
   },
   "outputs": [],
   "source": [
    "class MaxPool2x2:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: An ndarray containing input data, of shape (C, H, W)\n",
    "        Returns:\n",
    "        - out: Result of max pooling operation, of shape (C, H', W') where\n",
    "        H' = 1 + (H - pooling height) // stride\n",
    "        W' = 1 + (H - pooling width) // stride\n",
    "        \"\"\"\n",
    "        self.x = x  # cache\n",
    "        C, H, W = x.shape\n",
    "        out = np.zeros((C, H // 2, W // 2))\n",
    "\n",
    "        for c in range(C):\n",
    "            for i in range(H // 2):\n",
    "                for j in range(W // 2):\n",
    "                    out[c, i, j] = np.max(x[c, i * 2 : i * 2 + 2, j * 2 : j * 2 + 2])\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - dout: Upstream gradients, of shape (C, H', W')\n",
    "        Returns:\n",
    "        - dx: Gradient w.r.t input x\n",
    "        \"\"\"\n",
    "        C, H, W = self.x.shape\n",
    "\n",
    "        dx = np.zeros_like(self.x)  # C x H x W\n",
    "\n",
    "        for c in range(C):\n",
    "            for i in range(H // 2):\n",
    "                for j in range(W // 2):\n",
    "                    window = self.x[c, i * 2 : i * 2 + 2, j * 2 : j * 2 + 2]\n",
    "                    max_idx = np.argmax(window)\n",
    "                    # window.shape is (2, 2)\n",
    "                    max_r = max_idx // window.shape[1]\n",
    "                    max_c = max_idx % window.shape[1]\n",
    "                    dx[c, i * 2 : i * 2 + 2, j * 2 : j * 2 + 2][max_r, max_c] = dout[\n",
    "                        c, i, j\n",
    "                    ]\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwSHWO7aXtT3"
   },
   "source": [
    "#### (Aside) See it in action\n",
    "\n",
    "Below is the code that demonstrates how `forward()` and `backward()` work for the max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zT0otxySV3r"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "x_train = x_train[:, np.newaxis, :, :]\n",
    "x_test = x_test[:, np.newaxis, :, :]\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "img = x_train[0]\n",
    "label = y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1636925738999,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "4T1UldnUSoIV",
    "outputId": "dbbed0e3-657d-4e45-bc7b-5173d6c1292c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.3647059 , 0.9882353 , 0.99215686, 0.73333335],\n",
       "        [0.        , 0.9764706 , 0.99215686, 0.9764706 ],\n",
       "        [0.7176471 , 0.99215686, 0.99215686, 0.8117647 ],\n",
       "        [0.99215686, 0.99215686, 0.98039216, 0.7137255 ]]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[:, 16:20, 16:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1636925739622,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "oCw6Gu5rSlKc",
    "outputId": "5ebefa43-d0b4-448c-d6e9-263c754930e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.98823529, 0.99215686],\n",
       "        [0.99215686, 0.99215686]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = MaxPool2x2()\n",
    "out = pool.forward(img[:, 16:20, 16:20])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1636925740621,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "2V7exaxDXEVR",
    "outputId": "2c9d69f2-1298-4776-ccdd-49088e3520f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.9882353 , 0.99215686, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.99215686, 0.99215686, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = pool.backward(out)\n",
    "dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGoNZgqZxx2n"
   },
   "source": [
    "### Max pooling layer: forward pass\n",
    "\n",
    "During the forward pass, we pass in `x`, which really is $o$ from the previous convolutional layer. `x` will be a three-dimensional tensor of shape `C x H x W`. \n",
    "\n",
    "Here, we decided to simplify the code by specifying that this is a `2x2` max pooling layer. Handling different pool widths and heights isn't too difficult, and we'll leave this as an exercise for you. \n",
    "\n",
    "The forward pass code is simple. We simply iterate through each image region similarly to convolutional layers and take the max element in each region. The difference with the convolutional layer is that the image regions are non-overlapping. We also remember to cache the input `x` so that we can refer to it in the backpropagation phase. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLUPbmM3D_Ia"
   },
   "source": [
    "### Max pooling layer: backward pass\n",
    "\n",
    "During the backward pass, we receive `dout`, similar to convolutional layers. `dout` will represent the derivative of the loss with respect to the output of the max pooling layer and thus will be of shape `C x H x W`, where really `H` here is the original H divided by 3 (as a result of forward pass). \n",
    "\n",
    "Remember that max pooling layers have no learnable weights! Our aim with backpropagation here is to simply compute `dx`, so that we can pass it as `dout` for the previous layer (most likely a ReLU layer) so that gradients propagate through our entire model and weights are updated.\n",
    "\n",
    "Keeping this in mind, let's think about how to compute $\\frac{\\partial o}{\\partial x}$, where again we denote the result of max pooling as $o$ (remember that if we have  $\\frac{\\partial o}{\\partial x}$, we can compute $\\frac{\\partial L}{\\partial x}$ because we already have $\\frac{\\partial L}{\\partial o}$). During the forward pass, we take the max value in every `2 x 2` region. Therefore, intuitively, every other value in the region that is not the max will not change the output $o$. These values therefore have 0 gradient. For the max value, we simply assign the corresponding gradient value. Below is an illustration ([source](https://medium.com/@eternalzer0dayx/demystifying-convolutional-neural-networks-ca17bdc75559)):\n",
    "\n",
    "<img src='https://miro.medium.com/max/1098/0*jDAl5F-y3qzvcObd.png' width='500'/>\n",
    "\n",
    "The code we have written for backprop is simply translating this idea to code:   \n",
    "\n",
    "\n",
    "```python\n",
    "for c in range(C):\n",
    "      for i in range(H // 2):\n",
    "        for j in range(W // 2):\n",
    "          window = x[c, i*2:i*2 + 2, j*2:j*2 + 2]\n",
    "          max_idx = np.argmax(window)\n",
    "          max_r = max_idx // window.shape[1]\n",
    "          max_c = max_idx % window.shape[1]\n",
    "          dx[c, i*2:i*2+2, j*2:j*2+2][max_r, max_c] += dout[c, i, j]\n",
    "```\n",
    "\n",
    "One thing to keep in mind is that `np.argmax()` returns an index instead of (row, col) so we have to convert it appropriately, then index into it to incrementally build up `dx`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_o6mV_WdcHJ"
   },
   "source": [
    "## Softmax\n",
    "\n",
    "To complete our implementation of a CNN, we need a fully connected layer that flattens the tensor we get from the last conv layer (most likely with non-linearity and pooling applied) into a $k$-dimensional vector where $k$ is the number of classes. For our MNIST classifier, we have digits, so $k = 10$. \n",
    "\n",
    "We also apply a **softmax** activation to the predictions to convert them into probabilities that add up to 1. To convert these probabilities into a loss we can use for training, we use **cross-entropy loss**, given by the following formula:    \n",
    "\n",
    "\\begin{equation}\n",
    "L=−log(p_c)\n",
    "\\end{equation}\n",
    "\n",
    "where $p_c$ is the probability of the correct class. \n",
    "\n",
    "As this notebook is about CNNs, we won't get into too much detail for softmax / cross-entropy loss. Those who are interested can check out this [excellent blog post](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) from Eli Bendersky.\n",
    "\n",
    "Here are the implementations for a fully-connected linear layer as well as a function computing loss and derivatives for softmax loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwzdHlJbg5dg"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, weight_scale=1e-3):\n",
    "        # Initialise weight matrix of shape (in_features, out_features)\n",
    "        self.weights = np.random.randn(in_features, out_features) * weight_scale\n",
    "        self.bias = np.zeros(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: An ndarray containing input data, of shape (d_1, ..., d_k) such that\n",
    "          D = d_1 * ... * d_k is equal to `in_features`\n",
    "        Returns:\n",
    "        - out: output, of shape (1, out_features)\n",
    "        \"\"\"\n",
    "        self.x_before_flatten = x  # cache\n",
    "        reshaped_x = x.flatten()[np.newaxis, :]  # shape (1, D)\n",
    "        self.x_after_flatten = reshaped_x  # cache\n",
    "\n",
    "        # @ is the matrix multiplication operator in Python\n",
    "        # reshaped_x's shape: (1 , D)\n",
    "        # self.weight's shape: (D , M) where M is out_features\n",
    "        # result of reshaped_x @ self.weights is of shape (1, M), so (1,10) for MNIST\n",
    "        out = reshaped_x @ self.weights + self.bias\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - dout: An ndarray of the upstream derivatives, of shape (1, M) where M = out_features\n",
    "        Returns a tuple of:\n",
    "        - dx: Gradient w.r.t input x, of shape (d_1, ..., d_k)\n",
    "        - dw: Gradient w.r.t. weights w, of shape (D, M)\n",
    "        - db: Gradient w.r.t the biases, of shape (M, )\n",
    "        \"\"\"\n",
    "        dx = (dout @ self.weights.T).reshape(*self.x_before_flatten.shape)\n",
    "        dw = self.x_after_flatten.T @ dout  # (D x 1) * (1 x M)\n",
    "        db = dout.squeeze()  # turns (1, M) to (M, )\n",
    "        return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK_9KGVxOHl-"
   },
   "outputs": [],
   "source": [
    "def softmax_loss(scores, y):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "  - scores: ndarray of scores, of shape (N, C) where C is the no. of classes\n",
    "  - y: ndarray of class labels, of shape (N,) where y[i] is the label for x[i] \n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar value representing the softmax loss\n",
    "  - dscores: Gradient of loss w.r.t scores, of shape (N, C)\n",
    "  \"\"\"\n",
    "  loss = None, dscores = None\n",
    "  #----- Task 3 : Compute softmax loss and dx ---------#\n",
    "  # Hint: Implement https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ \n",
    "  # Remember to take care of numerical stability when doing exponentiation:\n",
    "  # See https://cs231n.github.io/linear-classify/#softmax-classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  #---------------- End of task 3 ---------------------#\n",
    "\n",
    "  return loss, dscores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZHBvj8_0YK8"
   },
   "source": [
    "### Putting it together\n",
    "\n",
    "Once we have the core components, we can put together the classes we've written so far to create a CNN model: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFkgg5E_1EZg"
   },
   "outputs": [],
   "source": [
    "conv = Conv(1, 8, 3)  # 28x28x1 -> 26x26x8\n",
    "pool = MaxPool2x2()  # 26x26x8 -> 13x13x8\n",
    "linear = Linear(13 * 13 * 8, 10)  # 13x13x8 -> 10\n",
    "\n",
    "\n",
    "def forward(image, y):\n",
    "    \"\"\"\n",
    "    Completes a forward pass of the CNN\n",
    "    - image is a numpy ndarray of shape (1 x 28 x 28) representing the greyscale image\n",
    "    - y is the corresponding digit\n",
    "    \"\"\"\n",
    "    # transform the image from [0, 255] to [0, 1]\n",
    "    out = conv.forward((image / 255))\n",
    "    out = pool.forward(out)\n",
    "    scores = linear.forward(out)\n",
    "\n",
    "    loss, dout = softmax_loss(scores, y)\n",
    "\n",
    "    return scores, loss, dout\n",
    "\n",
    "\n",
    "def loss(image, label):\n",
    "    \"\"\"\n",
    "    Computes loss + completes a backward pass of the whole model\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    # forward pass\n",
    "    scores, loss, dout = forward(image, label)\n",
    "\n",
    "    # Backprop\n",
    "    dpool, grads[\"W_linear\"], grads[\"b_linear\"] = linear.backward(dout)\n",
    "    dconv = pool.backward(dpool)\n",
    "    dx, grads[\"W_conv\"], grads[\"b_conv\"] = conv.backward(dconv)\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "# Challenge yourself: can you try to train the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNnc6nAe1DyI"
   },
   "source": [
    "\n",
    "In this notebook, we omitted the training part because we wanted to focus on what each component is doing — can you come up with the code to train the CNN model below on the MNIST dataset? Challenge yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri_J-WT_z_Jy"
   },
   "source": [
    "## CNN in Keras\n",
    "\n",
    "So far we've walked you through how to implement a CNN from scratch in Python. Fortunately, we practically never have to write a CNN from scratch nowadays, thanks to high-level libraries like Keras that help abstract all the underlying complexity away from us. To cap off this notebook, let's use Keras to implement the **same** CNN that we've implemented above. We adapted this example from the [official Keras MNIST CNN example](https://keras.io/examples/vision/mnist_convnet/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3411,
     "status": "ok",
     "timestamp": 1636994568800,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "WQRTH11M0Gun",
    "outputId": "6459a225-3236-4005-ae5e-b9ac0c28cbfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# split data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# scale images\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = x_train[:, :, :, np.newaxis]\n",
    "x_test = x_test[:, :, :, np.newaxis]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i75wVdMede0g"
   },
   "source": [
    "A few things worth pointing out in the code above:\n",
    "\n",
    "First, once we load the MNIST dataset, we scale the images so that each pixel is in the [0,1] range instead of [0,255]. This makes it easier for us to train the CNN.\n",
    "\n",
    "Next, we add a fourth dimension to the `x_train` and `x_test` tensors, which were initially of size `N x 28 x 28`, where `N` is the number of images. The last dimension we add represents the number of channels each image has, which is 1 because in MNIST we are working with greyscale images. For RGB, you would set this to 3. Also note that we add the number of channels as the last dimension in the Keras example. This is the standard adopted by Keras and Tensorflow. In our `numpy` implementation above, we adopted the convention used in `PyTorch`, the other major framework for machine learning, where the number of channels goes before the width and height of the image (i.e. a single image would be represented by a tensor of shape `1 x 28 x 28` instead of `28 x 28 x 1`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sLZJQxtfNsE"
   },
   "source": [
    "Now our images are ready for training! Now let's turn out attention to our labels.\n",
    "\n",
    "By default, `y_train` and `y_test` are arrays that contain single integers representing the digit that each image represents. Keras, however, expects each target label to be a **10-dimensional** vector. Therefore, we use `to_categorical()` from `keras.utils` to obtain an array of one-hot encoded vector. That is to say, if our target is 5, then after calling `to_categorical()`, we would get the zero-indexed array `[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]`.\n",
    "\n",
    "\n",
    "Here's the code, with some helpful outputs to see what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1636994572587,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "ishIxntsfLko",
    "outputId": "e684ccc5-d243-4a85-b85e-7a1b69d17eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[0] before to_categorical:  5\n",
      "y_train[0] after to_categorical:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "y_train shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding label vectors\n",
    "print(\"y_train[0] before to_categorical: \", y_train[0])\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"y_train[0] after to_categorical: \", y_train[0])\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPcgYLPugaXz"
   },
   "source": [
    "Once we've gotten our data ready for training, we now move onto actually specifying the model that we'll be using. Again, this is almost the exact same model that we've implemented from scratch moments ago (the only difference being the presence of ReLU activation following each `Conv2D`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5951,
     "status": "ok",
     "timestamp": 1636994579886,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "5q7FtcaibIEx",
    "outputId": "da75f7d7-44a4-4a61-d47e-1ad095910dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1352)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                13530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(filters=8, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmOwEarQhNKM"
   },
   "source": [
    "We also use a batch size of 1, which means we do gradient descent using gradients accumulated from just 1 image. We also do this for only 1 epoch. These settings are less than ideal, but to keep the Keras example consistent with our CNN implementation, we stick with it to see what kind of accuracy we can get. The [official Keras MNIST CNN example](https://keras.io/examples/vision/mnist_convnet/), for example, uses a batch size of `128` for 15 epochs with a deeper CNN model and easily achieves ~99% test accuracy on MNIST. Not bad!\n",
    "\n",
    "We use the Adam optimizer with the `categorical_crossentropy` loss that we've discussed previously, since we have 10 target classes (which is > 2). Keras also allows us to keep track of various metrics during the course of training, and here we specify that we want Keras to report the accuracy.\n",
    "\n",
    "Let's compile the model and train it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323275,
     "status": "ok",
     "timestamp": 1636994904150,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "RnOU1eVpbIko",
    "outputId": "0a1fc3ec-c181-4e08-a22a-a38bf2fa6d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000/54000 [==============================] - 266s 4ms/step - loss: 0.1660 - accuracy: 0.9506 - val_loss: 0.0787 - val_accuracy: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9d01f72d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "epochs = 1\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1636994905391,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "RrsqaVRIbOke",
    "outputId": "618c6302-ae85-4cfa-a0ae-d02ef90f6897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08045849204063416\n",
      "Test accuracy: 0.9753000140190125\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yJsQea1jdPS"
   },
   "source": [
    "After just 1 epoch of training, we obtain a test accuracy of ~97.5% on the MNIST dataset, which is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qadLyaFQkJpP"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "That's it! We've reached the end of this notebook. To recap, we walked you through how CNNs can be implemented from scratch in Python using just `numpy`, and we've seen how we can specify and train the same CNN model using **Keras** to reach a ~97.5% test accuracy on the MNIST dataset. Hopefully, the next time you use Keras or PyTorch or Tensorflow or any other similar libraries, remember that all they're doing at a high level is abstracting away the theory and code we've covered in this notebook (except they're vectorized / optimised for the GPU and thus often significantly faster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References / Helpful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://johnwlambert.github.io/conv-backprop/\n",
    "- https://www.youtube.com/watch?v=Lakz2MoHy6o&ab_channel=TheIndependentCode\n",
    "- https://victorzhou.com/blog/intro-to-cnns-part-2/\n",
    "- https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "- https://cs231n.github.io/linear-classify/#softmax-classifier\n",
    "- https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP17UHUfIDQ7gWVcVY4XcwM",
   "collapsed_sections": [
    "9x2ovXIWSpkc",
    "fwSHWO7aXtT3",
    "ri_J-WT_z_Jy",
    "qadLyaFQkJpP"
   ],
   "name": "CNN_from_scratch.ipynb",
   "provenance": [
    {
     "file_id": "1cweG3jT-rOy2_qpoWUeeNFLzi2mQTdAM",
     "timestamp": 1636655614454
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
